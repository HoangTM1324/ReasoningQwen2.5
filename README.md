# ReasoningQwen2.5
# Qwen 2.5 Reasoning Fine-tune ðŸ§ 

This project demonstrates how to fine-tune the **Qwen 2.5 (0.5B Instruct)** model to enhance its reasoning capabilities using **QLoRA** (Quantized Low-Rank Adaptation).

The model is trained on the **OpenThoughts-114k** dataset to improve Chain-of-Thought (CoT) generation and is evaluated on the **GSM8k** benchmark.

## ðŸš€ Key Features
* **Efficient Fine-tuning:** Uses 4-bit quantization (BitsAndBytes) and LoRA to train on consumer-grade GPUs.
* **Modular Codebase:** Logic is separated into reusable modules (`src/`) for better maintainability.
* **Reasoning Focus:** Specifically prompts the model to generate `<thinking>` steps before providing the solution.
* **Evaluation Pipeline:** Includes scripts to merge adapters and evaluate performance using `lm-evaluation-harness`.

## ðŸ“‚ Project Structure
```text
.
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_finetune_qwen.ipynb      # Main training notebook
â”‚   â””â”€â”€ 02_evaluate_model.ipynb     # Merging and benchmarking
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py                 # Package initialization
â”‚   â”œâ”€â”€ data_utils.py               # Dataset processing & formatting
â”‚   â”œâ”€â”€ model_utils.py              # Model loading & merging logic
â”‚   â””â”€â”€ trainer.py                  # LoRA config & Trainer setup
â”œâ”€â”€ requirements.txt                # Python dependencies
â””â”€â”€ README.md                       # Project documentation
